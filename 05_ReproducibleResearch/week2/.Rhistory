}
x<-c("a","b","c","d","e")
fori=1:5{
print(x[i])
}
a=0
for i=1 in 10{
if (a<3){
y[i]<-1
}else{
y[i]<-0
}
a<-a+1
}
x<-c("a","b","c","d","e")
fori=1 in 5{
print(x[i])
}
a=0
for i in 1:10{
if (a<3){
y[i]<-1
}else{
y[i]<-0
}
a<-a+1
}
x<-c("a","b","c","d","e")
fori in 1:5{
print(x[i])
}
x<-c("a","b","c","d","e")
for i in 1:5{
print(x[i])
}
x<-c("a","b","c","d","e")
for (i in 1:5){
print(x[i])
}
a=0
for (i in 1:10){
if (a<3){
y[i]<-1
}else{
y[i]<-0
}
print(y[i])
a<-a+1
}
x<-matrix(1:6,2,3)
for (i in seq_len(nrow(x)))
for (j in seq_len(ncol(x)))
print(x[i,j])
end
end
x<-matrix(1:6,2,3)
for (i in seq_len(nrow(x))){
for (j in seq_len(ncol(x))){
print(x[i,j])
}
}
x
x<-rbinom(1,1.4,.5)
x<-rbinom(1,1.4,0.5)
rbinom
z<-3
while(z<=5 && z>=2){
print(z)
if coin = rbinom(1,1,.5){
z<-z+1
}else{
z<-z-1
}
}
z<-3
while(z<=5 && z>=2){
print(z)
coin = rbinom(1,1,.5)
if coin==1{
z<-z+1
}else{
z<-z-1
}
}
z<-3
while(z<=5 && z>=2){
print(z)
coin = rbinom(1,1,.5)
if (coin==1){
z<-z+1
}else{
z<-z-1
}
}
add2(1,2)
add2(1,2)
add2(1,2)
add2 <- function(x,y){
x+y
}
add2(2)
add2(2,1)
source('C:/Users/lenovo/Desktop/add2.R')
source('C:/Users/lenovo/Desktop/add2.R')
add2(1,2)
source('~/.active-rstudio-document')
x=c(10,20,3,9)
above(x,4)
above(x,10)
source('~/.active-rstudio-document')
columnmean(airquality)
columnmean(airquality)
columnmean(airquality)
source('C:/Users/lenovo/Desktop/add2.R')
columnmean(airquality)
source('C:/Users/lenovo/Desktop/add2.R')
columnmean(airquality)
mydata
mydata=rnorm(100)
source('~/.active-rstudio-document')
f(2,3)
args(paste)
args(cat)
?rnorm
a<-available.packages()
install.packages(KernSmooth)
install.packages("KernSmooth")
library(KernSmooth)
git commit
install.packages("RMySQL")
install.packages('RMySQL',type='source')
library(httr)
setwd("C:\Users\lenovo\Desktop\Rlearning\Getting and Cleaning Data")
setwd("C:\\Users\\lenovo\\Desktop\\Rlearning\\Getting and Cleaning Data")
library(httr)
source('C:/Users/lenovo/Desktop/Rlearning/Getting and Cleaning Data/week2/getting2.R')
source('C:/Users/lenovo/Desktop/Rlearning/Getting and Cleaning Data/week2/getting2.R')
url<-https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv
url<-"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileUrl, destfile = ".//week2//data1quiz2.csv")
download.file(url, destfile = ".//week2//data1quiz2.csv")
acs<- read.csv("C:/Users/lenovo/Desktop/Rlearning/Getting and Cleaning Data/week2/getdata_data_ss06pid.csv")
acs
source('C:/Users/lenovo/Desktop/Rlearning/Getting and Cleaning Data/week2/getting2.R')
acs<- read.csv("C:/Users/lenovo/Desktop/Rlearning/Getting and Cleaning Data/week2/getdata_data_ss06pid.csv")
names(acs)
sqldf("select pwgtp1 from acs where AGEP < 50")
library(MySQL)
library(RMySQL)
sqldf("select pwgtp1 from acs where AGEP < 50")
library(sqldf )
library(sqldf)
sqldf("select pwgtp1 from acs where AGEP < 50")
sqldf("select pwgtp1 from acs where AGEP < 50")
sqldf("select pwgtp1 from acs")
sqldf("select pwgtp1 from acs")
a<-sqldf("select pwgtp1 from acs")
a
library(sqldf)
install.packages("sqldf)
install.packages("sqldf")
install.packages("sqldf")
install.packages("sqldf")
install.packages("sqldf")
library(httr)
GET("http://google.com/")
con = url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en" )
htmlCode = readLines(con)
close(con)
htmlCode
library(XML)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url, useInternalNodes= T)
html <- htmlTreeParse(url, useInternalNodes= T)
xpathSApply(html, "//title", xmlValue)
xpathSApply(html, "//td[@id='col-citedby']" , xmlValue)
html <- htmlTreeParse(url, useInternalNodes= T)
xpathSApply(html, "//td[@id='col-citedby']" , xmlValue)
xpathSApply(html, "//title", xmlValue)
xpathSApply(html, "//td[@id='col-citedby']" , xmlValue)
a<-xpathSApply(html, "//td[@id='col-citedby']" , xmlValue)
a
library(httr); html2 = GET(url)
content2 = content(html2,as= "text")
parsedHtml = htmlParse(content2,asText= TRUE)
xpathSApply(parsedHtml, "//title", xmlValue)
pg2 = GET("http://httpbin.org/basic-auth/user/passwd" ,
authenticate( "user","passwd"))
pg2
names(pg2)
google = handle( "http://google.com" )
pg1 = GET(handle=google,path= "/")
pg2 = GET(handle=google,path= "search")
pg1
pg2
names(pg2)
library(httr)
oauth_endpoints("github")
myapp <- oauth_app("github", "56b637a5baffac62cad9")
myapp <- oauth_app("github", "9e674afb3f37971db6f8f729e398726968b13252")
myapp <- oauth_app("github", "43cb305e93285a9d519a")
google = handle( "http://google.com" )
myapp = oauth_app( "twitter",key= "yourConsumerKeyHere" ,secret="yourConsumerSecretHere" )
sig = sign_oauth1.0(myapp,token = "yourTokenHere" ,token_secret = "yourTokenSecretHere" )
homeTL = GET( "https://api.twitter.com/1.1/statuses/home_timeline.json" , sig)
myapp = oauth_app( "twitter",key= "yourConsumerKeyHere" ,secret="yourConsumerSecretHere" )
sig = sign_oauth1.0(myapp,token = "yourTokenHere" ,token_secret = "yourTokenSecretHere" )
homeTL = GET( "https://api.twitter.com/1.1/statuses/home_timeline.json" , sig)
json1 = content(homeTL)
myapp = oauth_app( "github",key= "43cb305e93285a9d519a" ,secret="9e674afb3f37971db6f8f729e398726968b13252" )
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
gtoken <- config(token = github_token)
myapp <- oauth_app("github",key="43cb305e93285a9d519a",secret = "9e674afb3f37971db6f8f729e398726968b13252") github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
myapp <- oauth_app("github",key="43cb305e93285a9d519a",secret = "9e674afb3f37971db6f8f729e398726968b13252")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
gtoken <- config(token = github_token)
req <- GET("https://api.github.com/rate_limit", gtoken)
stop_for_status(req)
content(req)
req <- GET(https://api.github.com/users/jtleek/repos, gtoken)
req <- GET("https://api.github.com/users/jtleek/repos", gtoken)
stop_for_status(req)
content(req)
req$datasharing
content(req)
req
content2 = content(req,as= "datasharing")
content2 = content(req,as= "text")
content2
content2 = content(req,as= "date")
content2 = content(req,as= "raw")
content2
content2 = content(req,as= "raw2")
content2 = content(req,as= "parsed")
content2
content2["datasharing"]
content2["date created"]
content2["data created"]
content2["datacreated"]
content2[data created]
content2["datacreated"]
content["datacreated"]
content2["datacreated"]
acs<- read.csv("C:/Users/lenovo/Desktop/Rlearning/Getting and Cleaning Data/week2/getdata_data_ss06pid.csv")
library(RMySQL)
ucscDb <- dbConnect(MySQL(),user= "genome", host= "genome-mysql.cse.ucsc.edu" )
esult <- dbGetQuery(ucscDb, "show databases;" ); dbDisconnect(ucscDb);
ucscDb <- dbConnect(MySQL(),user= "genome", host= "genome-mysql.cse.ucsc.edu" )
result <- dbGetQuery(ucscDb, "show databases;" ); dbDisconnect(ucscDb);
result
hg19 <- dbConnect(MySQL(),user= "genome", db="hg19", host= "genome-mysql.cse.ucsc.edu" )
allTables <- dbListTables(hg19)
length(allTables)
allTables
allTables[1:5]
dbListFields(hg19, "affyU133Plus2" )
dbListFields(hg19, "affyU133Plus2" )
dbGetQuery(hg19, "select count(*) from affyU133Plus2" )
affyData <- dbReadTable(hg19, "affyU133Plus2" )
save(affyData, file = "affyData.RData")
query <- dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3" )
affyMis <- fetch(query); quantile(affyMis$misMatches)
affyMisSmall <- fetch(query,n= 10); dbClearResult(query);
v
dim(affyMisSmall)
dbDisconnect(hg19)
dbDisconnect(ucscDb)
dbDisconnect(ucscDb);
library(sqldf )
library(sqldf)
acs<- read.csv("C:/Users/lenovo/Desktop/Rlearning/Getting and Cleaning Data/week2/getdata_data_ss06pid.csv")
sqldf("select * from acs")
sqldf("select * from acs where AGEP < 50")
sqldf("select pwgtp1 from acs where AGEP < 50")
sqldf("select pwgtp1 from acs")
??sqldf
install.packages("C:/Users/lenovo/Desktop/pack/RMySQL_0.8-0.tar.gz", repos = NULL, type = "source")
unique(acs$AGEP)
sqldf("select distinct AGEP from acs")
library(RMySQL)
library(XML)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
url <- "
http://biostat.jhsph.edu/~jleek/contact.html
"
url <- "http://biostat.jhsph.edu/~jleek/contact.html"
url <- "http://biostat.jhsph.edu/~jleek/contact.html"
html <- htmlTreeParse(url, useInternalNodes= T)
html
xpathSApply(html, "//title", xmlValue)
a<-xpathSApply(html, xmlValue)
nchar(html[[10]])
nchar(html[10])
nchar(html[10,])
html[10,]
x<-content(html)
library(httr)
html=GET(url)
content2=content(html,as"text")
content2<-content(html,as"text")
content2<-content(html,as="text")
content2
cntent2[10]
content2[10]
content2[[10]
content2[[10]]
content2[[10]]
parsed<-htmlParse(content2,asText=T)
parsed
parsed[10]
parsed[[10]
parsed[[10]]
nchar(content2)
nchar(content2[10])
nchar(content2[20])
tables=readHTMLTable(doc="117E.html",header=T,as.data.frame=T)
url<- "https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"
html <- htmlTreeParse(url, useInternalNodes= T)
library(httr)
html2 = GET(url)
content2 = content(html2,as= "text")
content2
content2 = content(html2,as= "row")
content2 = content(html2,as= "raw")
content2
content2 = content(html2,as= "c")
content2 = content(html2,as= "parsed")
content2 = content(html2,as= "parsed")
…or a shorter try:
library(XML)
theurl <- "http://en.wikipedia.org/wiki/Brazil_national_football_team"
tables <- readHTMLTable(theurl)
n.rows <- unlist(lapply(tables, function(t) dim(t)[1]))
the picked table is the longest one on the page
tables[[which.max(n.rows)]]
shareimprove this answer
answered Dec 4 '09 at 20:14
user225056
…or a shorter try:
library(XML)
theurl <- "http://en.wikipedia.org/wiki/Brazil_national_football_team"
tables <- readHTMLTable(theurl)
n.rows <- unlist(lapply(tables, function(t) dim(t)[1]))
the picked table is the longest one on the page
tables[[which.max(n.rows)]]
shareimprove this answer
answered Dec 4 '09 at 20:14
user225056
library(XML)
theurl <- "http://en.wikipedia.org/wiki/Brazil_national_football_team"
tables <- readHTMLTable(theurl)
tables <- readHTMLTable(url)
tables
theurl <- "http://www.cpc.ncep.noaa.gov/data/indices/wksst8110.for"
tables <- readHTMLTable(theurl)
tables
tables[1]
tables[[1]
tables[[1]]
tables[[1]]
theurl <- "http://www.cpc.ncep.noaa.gov/data/indices/wksst8110.for"
theurl <- "http://biostat.jhsph.edu/~jleek/contact.html"
tables <- readHTMLTable(theurl)
tables[1]
tables[2]
tables
n.rows <- unlist(lapply(tables, function(t) dim(t)[1]))
library(httr); html2 = GET(theurl)
content2 = content(html2,as= "text")
content2
content2[1]
content2[2]
content2[[2]]
content2[1,]
parsedHtml = htmlParse(content2,asText= TRUE)
parseHtml
parsedHtml
parsedHtml[[1]]
xmlValue(parsedHtml)
tables = readHTMLTable(theurl)
tables[[1]]
tables[1]
tables[2]
tables[10]
tables[20]
tables
tables = readHTMLTable(parsedHtml)
tables
html <- htmlTreeParse(theurl, useInternalNodes= T)
hurl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
f <- file.path(getwd(), "ss06pid.csv")
download.file(url, f)
acs <- data.table(read.csv(f))
query1 <- sqldf("select pwgtp1 from acs where AGEP < 50")
setwd("C:\\Users\\lenovo\\Desktop\\Rlearning\\exploratory data analysis\\week2")
xyplot(y ~ x | f * g, data)
library(latice)
library(lattice)
xyplot(y ~ x | f * g, data)
library(lattice)
library(datasets)
## Simple scatterplot
xyplot(Ozone ~ Wind, data = airquality)
library(datasets)
library(lattice)
## Convert 'Month' to a factor variable
airquality <- transform(airquality, Month = factor(Month))
xyplot(Ozone ~ Wind | Month, data = airquality, layout = c( 5, 1))
p <- xyplot(Ozone ~ Wind, data = airquality) ## Nothing happens!
print(p) ## Plot appears
set.seed(10)
x <- rnorm(100)
f <- rep(0:1, each = 50)
y <- x + f - f * x + rnorm(100, sd = 0.5)
f <- factor(f, labels = c( "Group 1", "Group 2"))
xyplot(y ~ x | f, layout = c( 2, 1)) ## Plot with 2 panels
xyplot(y ~ x | f, panel = function(x, y, ...) {
panel.xyplot(x, y, ...) ## First call the default panel function for 'xyplot'
panel.abline(h = median(y), lty = 2) ## Add a horizontal line at the median
})
## Custom panel function
xyplot(y ~ x | f, panel = function(x, y, ...) {
panel.xyplot(x, y, ...) ## First call default panel function
panel.lmline(x, y, col = 2) ## Overlay a simple linear regression line
})
library(ggplot)
library(ggplot2)
qplot(displ,hwy,data=mpg,color=drv)
qplot(logpm25, NocturnalSympt, data = maacs, facets = . ~ bmicat, geom =c("point", "smooth"), method = "lm”)
)
qplot(logpm25, NocturnalSympt, data = maacs, facets = . ~ bmicat, geom =c("point", "smooth"), method = "lm”))
debugSource('C:/Users/lenovo/Desktop/Rlearning/exploratory data analysis/week2/exploratory.R')
debugSource('C:/Users/lenovo/Desktop/Rlearning/exploratory data analysis/week2/exploratory.R')
debugSource('C:/Users/lenovo/Desktop/Rlearning/exploratory data analysis/week2/exploratory.R')
debugSource('C:/Users/lenovo/Desktop/Rlearning/exploratory data analysis/week2/exploratory.R')
debugSource('C:/Users/lenovo/Desktop/Rlearning/exploratory data analysis/week2/exploratory.R')
debugSource('C:/Users/lenovo/Desktop/Rlearning/exploratory data analysis/week2/exploratory.R')
debugSource('C:/Users/lenovo/Desktop/Rlearning/exploratory data analysis/week2/exploratory.R')
debugSource('C:/Users/lenovo/Desktop/Rlearning/exploratory data analysis/week2/exploratory.R')
qplot(hwy,data=mpg,facets=drv~.,binwidth=2)
qplot(log(eno),data=maacs)
library(ggplot2)
qplot(displ, hwy,data=mpg,color	=drv)
library(ggplot2)
qplot(displ, hwy,data=mpg,color	=drv)
qplot(displ,hwy,data=mpg,geom=c("point","smooth"))
qplot(hwy,data=mpg,fill=drv)
qplot(displ,hwy,data=mpg,facets=.~drv)
qplot(hwy,data=mpg,facets=drv~.,binwidth=2)
qplot(log(eno),data=maacs)
qplot(log(eno),data=maacs,fill=mopos)
qplot(log(eno),data=maacs,geom="density")
str(maacs)
library(ggplot2)
str(maacs)
library(maacs)
dataset(maacs)
library(datasets)
maacs
library(datasets)
qplot(log(eno),data=maacs)
??maacs
library(datasets)
qplot(log(eno),data=mpg)
qplot(log(eno),data=mpg,fill=mopos)
library(ggplot2)
qplot(log(eno),data=mpg)
qplot(log(eno),data=mpg,fill=mopos)
qplot(log(eno),data=mpg,geom="density")
qplot(log(eno),data=mpg,geom="density",color=mopos)
qplot(log(pm25),log(eno),data=mpg)
qplot(log(pm25),log(eno),data=mpg,shape=mopos)
qplot(log(pm25),log(eno),data=mpg,color=mopos)
qplot(displ, hwy,data=mpg,color	=drv)
qplot(log(eno),data=mpg,fill=mopos)
head(maacs)
head(mpg)
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
library(lattice)
library(datasets)
data(airquality)
p <- xyplot(Ozone ~ Wind | factor(Month), data = airquality)
print(p)
library(datasets)
data(airquality)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
library(ggplot2)
library(datasets)
data(airquality)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
setwd(C:\\Users\\lenovo\\Desktop\\Rlearning\\reproducible research\\week2)
setwd("C:\\Users\\lenovo\\Desktop\\Rlearning\\reproducible research\\week2)
setwd("C:\\Users\\lenovo\\Desktop\\Rlearning\\reproducible research\\week2")
setwd("C:\\Users\\lenovo\\Desktop\\Rlearning\\reproducible research\\week2")
rm(ls==)
rm(ls())
ls()
rm(list=ls())
r-rm(list=ls())
rm(list=ls())
a<-1
rm(list=ls())
install.packages("C:/Users/lenovo/Desktop/pack/rmarkdown_0.5.1.zip", repos = NULL)
install.packages("C:/Users/lenovo/Desktop/pack/rmarkdown_0.5.1.zip", repos = NULL)
When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
title: "test"
====================================================
